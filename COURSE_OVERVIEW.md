# LLM全流程实战：从微调到部署

## 课程大纲（2天，每天6小时）

---

## **Day 1: 模型微调与量化**

### **上午 (3小时)**

#### **模块1: LLM全景概览** (45分钟)
- LLM发展简史与技术栈全景图
- 本课程技术栈定位：LLaMA Factory → llama.cpp → Ollama → One-API
- Qwen/DeepSeek模型家族介绍
- **实验环境准备**：AutoDL环境配置、镜像选择、依赖安装

#### **模块2: LLaMA Factory微调实战** (2小时15分钟)
**理论部分** (45分钟)
- 微调的本质：为什么需要微调？
- 微调方法对比：Full Fine-tuning vs LoRA vs QLoRA
- 数据准备：数据格式、质量要求、常见问题
- 超参数解析：学习率、batch size、epoch等

**实操部分** (1小时30分钟)
- **实验1-1**: LLaMA Factory界面熟悉（15分钟）
  - WebUI启动与配置
  - 模型下载与加载（Qwen2.5-7B-Instruct）
- **实验1-2**: LoRA微调实战（45分钟）
  - 使用示例数据集进行微调
  - 监控训练过程（loss曲线、显存占用）
  - 模型合并与导出
- **实验1-3**: 微调效果测试（30分钟）
  - 对比原模型与微调后模型
  - 常见问题排查

---

### **下午 (3小时)**

#### **模块3: 模型量化原理与实践** (2小时)
**理论部分** (40分钟)
- 量化的必要性：精度vs效率的权衡
- 量化技术详解：
  - INT8/INT4/GGUF格式对比
  - 对称量化vs非对称量化
  - 量化感知训练vs训练后量化
- llama.cpp架构与GGUF格式深度解析

**实操部分** (1小时20分钟)
- **实验2-1**: llama.cpp环境搭建（20分钟）
  - 编译llama.cpp
  - 工具链熟悉
- **实验2-2**: 模型量化实战（40分钟）
  - 将微调模型转换为GGUF格式
  - 不同量化级别对比（Q4_K_M, Q5_K_M, Q8_0）
  - 量化前后效果评估
- **实验2-3**: llama.cpp推理测试（20分钟）
  - 命令行推理
  - 性能指标分析（tokens/s、显存占用）

#### **模块4: Ollama本地部署** (1小时)
**理论部分** (20分钟)
- Ollama架构设计理念
- Modelfile详解
- Ollama vs 其他部署方案对比

**实操部分** (40分钟)
- **实验3-1**: Ollama安装与配置（15分钟）
- **实验3-2**: 导入自定义模型（25分钟）
  - 编写Modelfile
  - 导入量化后的模型
  - 测试对话功能
  - 系统提示词优化

---

## **Day 2: 模型管理与生产部署**

### **上午 (3小时)**

#### **模块5: Ollama进阶与模型管理** (1小时30分钟)
**理论部分** (30分钟)
- Ollama API详解（OpenAI兼容接口）
- 模型版本管理最佳实践
- 多模型并存策略

**实操部分** (1小时)
- **实验4-1**: Ollama API调用（30分钟）
  - Python客户端调用
  - 流式输出实现
  - 参数调优（temperature、top_p等）
- **实验4-2**: 部署多个模型（30分钟）
  - 部署DeepSeek-V3模型
  - 模型切换与对比
  - 资源监控

#### **模块6: One-API统一管理平台** (1小时30分钟)
**理论部分** (30分钟)
- API网关的作用与价值
- One-API架构设计
- 令牌管理、渠道管理、用户管理概念
- **RAG概念预告**：嵌入模型的作用（为后续课程埋钩子）

**实操部分** (1小时)
- **实验5-1**: One-API部署（20分钟）
  - Docker部署One-API
  - 初始配置
- **实验5-2**: 接入Ollama模型（20分钟）
  - 添加渠道
  - 创建令牌
  - 测试调用
- **实验5-3**: 嵌入模型部署（20分钟）
  - 在Ollama中部署embedding模型（如bge-large-zh）
  - 通过One-API调用
  - 演示向量生成（为RAG铺垫）

---

### **下午 (3小时)**

#### **模块7: 生产环境最佳实践** (1小时30分钟)
**理论部分** (45分钟)
- 性能优化策略：
  - 批处理优化
  - KV Cache管理
  - 并发控制
- 监控与日志：
  - 关键指标监控
  - 日志分析
- 成本控制：
  - 显存优化技巧
  - 模型选择策略
- 安全考虑：
  - API密钥管理
  - 内容过滤
  - 速率限制

**实操部分** (45分钟)
- **实验6-1**: 性能压测（30分钟）
  - 使用工具进行并发测试
  - 分析性能瓶颈
  - 调优配置
- **实验6-2**: 监控配置（15分钟）
  - 配置Prometheus监控（可选）
  - 查看关键指标

#### **模块8: 综合实战与答疑** (1小时30分钟)
**实战演练** (1小时)
- **综合实验**: 完整流程串联
  - 场景：构建一个客服助手
  - 步骤：
    1. 准备客服对话数据集
    2. 微调Qwen模型
    3. 量化并部署到Ollama
    4. 通过One-API提供服务
    5. 编写简单的客户端程序

**答疑与总结** (30分钟)
- 常见问题解答
- 课程回顾
- 学习路径建议

---

## **可选课后项目**

### **项目：构建个性化AI助手**

**目标**: 巩固所学知识，完成端到端实践

**任务**:
1. 选择一个垂直领域（如法律咨询、编程助手、写作助手）
2. 收集或构造100-500条高质量训练数据
3. 使用LLaMA Factory微调Qwen或DeepSeek模型
4. 对比不同量化级别的效果
5. 部署到Ollama并通过One-API提供服务
6. 编写一个简单的Web界面或命令行工具

**交付物**:
- 训练数据集
- 微调配置文件
- 量化后的模型
- 部署文档
- 测试报告（包含效果对比）

**评估标准**:
- 数据质量（30%）
- 模型效果（30%）
- 部署完整性（20%）
- 文档质量（20%）

---

## **技术栈版本建议**
- **LLaMA Factory**: 最新稳定版
- **llama.cpp**: 最新release
- **Ollama**: 0.1.x系列
- **One-API**: 最新版本
- **基础模型**:
  - Qwen2.5-7B-Instruct
  - DeepSeek-V3（根据显存情况选择合适版本）
- **Embedding模型**: bge-large-zh-v1.5